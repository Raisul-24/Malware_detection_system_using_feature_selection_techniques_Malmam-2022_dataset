# -*- coding: utf-8 -*-
"""MalwareDetectionSystem_feature_selection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Sp1c9mfUkKJTPHcMgXIDf7a3DLXrwRw4
"""

# Import necessary libraries
from sklearn.feature_selection import RFE
from xgboost import XGBClassifier
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.decomposition import PCA

# Load the dataset
from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv('/content/drive/MyDrive/Malware Detection /dataset/Obfuscated-MalMem2022.csv')

import seaborn as sns
# Compute the correlation matrix
correlation_matrix = data.corr()
# Create a heatmap without displaying values
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm')
plt.title('Correlation Heatmap from CIC-Malware 2022')
plt.show()

# Preprocessing
data.dropna(inplace=True)
X = data.drop('Class', axis=1)
y = data['Class']

# Set the random state
random_state = 10

# Split dataset into train and test
train, test = train_test_split(data, test_size=0.3, random_state=random_state)

# One-hot encoding
#Exploratory Analysis
# Descriptive statistics
train.describe()
test.describe()

# Packet Attack Distribution
train['Class'].value_counts()
test['Class'].value_counts()
#Scalling numerical attributes
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

# extract numerical attributes and scale it to have zero mean and unit variance
cols = train.select_dtypes(include=['float64','int64']).columns
sc_train = scaler.fit_transform(train.select_dtypes(include=['float64','int64']))
sc_test = scaler.fit_transform(test.select_dtypes(include=['float64','int64']))

# turn the result back to a dataframe
sc_traindf = pd.DataFrame(sc_train, columns = cols)
sc_testdf = pd.DataFrame(sc_test, columns = cols)

# importing one hot encoder from sklearn
from sklearn.preprocessing import OneHotEncoder

# creating one hot encoder object
onehotencoder = OneHotEncoder()

trainDep = train['Class'].values.reshape(-1,1)
trainDep = onehotencoder.fit_transform(trainDep).toarray()
testDep = test['Class'].values.reshape(-1,1)
testDep = onehotencoder.fit_transform(testDep).toarray()

train_X=sc_traindf
train_y=trainDep[:,0]

test_X=sc_testdf
test_y=testDep[:,0]

# Create a Random Forest Classifier
rfc = RandomForestClassifier(n_estimators=100, random_state=random_state)
rfc.fit(train_X, train_y)

# Get feature importances from RFE
feature_importance_rfc = rfc.feature_importances_

# Get the indices of the top 4 most important features
top_feature_indices = feature_importance_rfc.argsort()[::-1][:4]

print("Selected Features from RFC:")
for i, index in enumerate(top_feature_indices, start=1):
    feature_name = train_X.columns[index]
    print(f"{i}. {feature_name}")

from sklearn.feature_selection import RFE
# Create an RFE model
rfe = RFE(rfc, n_features_to_select=4)
rfe.fit(train_X, train_y)

# Get the selected features
selected_features_rfe = train_X.columns[rfe.support_].tolist()
print("Selected Features from RFE:", selected_features_rfe)

import xgboost as xgb

# Create and fit an XGBoost classifier
xgb_classifier = xgb.XGBClassifier(random_state=random_state)
xgb_classifier.fit(train_X, train_y)

# Get feature importances from XGBoost
feature_importance_xgb = xgb_classifier.feature_importances_

# Get the indices of the top 4 most important features
top_feature_indices = feature_importance_xgb.argsort()[::-1][:4]

print("Selected Features from XGBoost:")
for i, index in enumerate(top_feature_indices, start=1):
    feature_name = train_X.columns[index]
    print(f"{i}. {feature_name}")

# Create an SelectKBest model
selectKBest_classifier = SelectKBest(score_func=f_classif, k=4)
selectKBest_classifier.fit(train_X, train_y)

# Feature selection using ANOVA F-statistic (SelectKBest)
selected_features_skb = train_X.columns[selectKBest_classifier.get_support()].tolist()
print("Selected Features from SelectKBest (ANOVA F-statistic):", selected_features_skb)

# Create a DataFrame for feature importances
importances_df = pd.DataFrame({
    'Feature': train_X.columns,
    'Random Forest': feature_importance_rfc,
    'RFE': [1 if feature in selected_features_rfe else 0 for feature in train_X.columns],
    'XGBoost': feature_importance_xgb,
    'SelectKBest': [1 if feature in selected_features_skb else 0 for feature in train_X.columns]
})

# Sort the DataFrame by descending order of Random Forest importance
importances_df = importances_df.sort_values(by='Random Forest', ascending=False)

# Create a figure with subplots
fig, axes = plt.subplots(1, 4, figsize=(18, 6))

# Plot Random Forest feature importances
sns.barplot(x='Random Forest', y='Feature', data=importances_df.head(10), ax=axes[0])
axes[0].set_title('Random Forest Feature Importance (Top 10)')
axes[0].set_xlabel('Feature Importance')
axes[0].set_ylabel('Feature')

# Plot RFE selected features
sns.barplot(x='RFE', y='Feature', data=importances_df.head(10), ax=axes[1])
axes[1].set_title('RFE Selected Features (Top 10)')
axes[1].set_xlabel('Selected (1) / Not Selected (0)')
axes[1].set_ylabel('Feature')

# Plot XGBoost feature importances
sns.barplot(x='XGBoost', y='Feature', data=importances_df.head(10), ax=axes[2])
axes[2].set_title('XGBoost Feature Importance (Top 10)')
axes[2].set_xlabel('Feature Importance')
axes[2].set_ylabel('Feature')

# Plot selectKBestt feature importances
sns.barplot(x='SelectKBest', y='Feature', data=importances_df.head(10), ax=axes[3])
axes[3].set_title('selectKBest Feature Importance (Top 10)')
axes[3].set_xlabel('Selected (1) / Not Selected (0)')
axes[3].set_ylabel('Feature')

# Adjust layout and show
plt.tight_layout()
plt.show()

# Extract feature importances
feature_importance_rfc = rfc.feature_importances_

# extract important features
score = np.round(rfc.feature_importances_,3)
importances = pd.DataFrame({'feature':train_X.columns,'importance':score})
importances = importances.sort_values('importance',ascending=False).set_index('feature')
# plot importances
plt.rcParams['figure.figsize'] = (11, 4)
importances.plot.bar();
plt.title('Feature Importance for RFE Techniques')

# Extract feature importances
feature_importance_xgb = xgb_classifier.feature_importances_
# extract important features
score = np.round(xgb_classifier.feature_importances_,3)
importances = pd.DataFrame({'feature':train_X.columns,'importance':score})
importances = importances.sort_values('importance',ascending=False).set_index('feature')
# plot importances
plt.rcParams['figure.figsize'] = (11, 4)
importances.plot.bar();
plt.title('Feature Importance for XGBoost Techniques')

import matplotlib.pyplot as plt
import numpy as np

# Feature importances from RFE and XGBoost
feature_importance_rfe = rfc.feature_importances_  # Replace with your actual RFE feature importances
feature_importance_xgb = xgb_classifier.feature_importances_  # Replace with your actual XGBoost feature importances

# Get bar names
bar_names = train_X.columns.tolist()

# Sort together by importance for RFE
sorted_idx_rfe = np.argsort(feature_importance_rfe)
feature_importance_rfe = feature_importance_rfe[sorted_idx_rfe]
bar_names_rfe = [bar_names[i] for i in sorted_idx_rfe]

# Sort together by importance for XGBoost
sorted_idx_xgb = np.argsort(feature_importance_xgb)
feature_importance_xgb = feature_importance_xgb[sorted_idx_xgb]

# Set up positions for bars
x = np.arange(len(bar_names))

# Plot the bar plot
plt.figure(figsize=(12, 6))
plt.bar(x, feature_importance_rfe, label='RFE', alpha=0.7)
plt.bar(x, feature_importance_xgb, label='XGBoost', alpha=0.7)
plt.xlabel('Feature')
plt.ylabel('Feature Importance')
plt.title('Comparison of Feature Importance Techniques')
plt.xticks(x, bar_names_rfe, rotation=90)  # Rotate x-axis labels for readability
plt.legend()
plt.tight_layout()
plt.show()

from sklearn import tree
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
# Standardize the features
scaler = StandardScaler()
train_X = scaler.fit_transform(train_X)
test_X = scaler.transform(test_X)

# Create models
models = {
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=random_state),
    'XGBoost': XGBClassifier(random_state=random_state),
    'Gradient Boosting': GradientBoostingClassifier(),
    'SVM': SVC(),
    'Logistic Regression': LogisticRegression(),
    'KNN Classifier': KNeighborsClassifier(),
    'Decision Tree': DecisionTreeClassifier()
}

# Train and evaluate models
for name, model in models.items():
    model.fit(train_X,train_y)
    y_pred = model.predict(test_X)

    accuracy = accuracy_score(test_y, y_pred)
    report = classification_report(test_y, y_pred)

    print(f'{name}:\nAccuracy: {accuracy*100:.3f}%\nClassification Report:\n{report}\n{"="}\n')

# Visualise models
import seaborn as sns
# Train and evaluate models
results = []
for name, model in models.items():
    model.fit(train_X,train_y)
    y_pred = model.predict(test_X)

    accuracy = accuracy_score(test_y, y_pred)
    report = classification_report(test_y, y_pred, output_dict=True                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               )

    results.append({
        'Model': name,
        'Accuracy': accuracy,
        'Precision': report['weighted avg']['precision'],
        'Recall': report['weighted avg']['recall'],
        'F1-Score': report['weighted avg']['f1-score']
    })

# Create a DataFrame for results
results_df = pd.DataFrame(results)

# Plotting
plt.figure(figsize=(10, 6))
sns.barplot(x='Model', y='Accuracy', data=results_df)
plt.title('Model Comparison for Malware Detection')
plt.ylim(0, 1)  # Set y-axis limit for accuracy
plt.ylabel('Accuracy')
plt.xticks(rotation=45)
plt.tight_layout()

plt.show()

# Create a confusion matrix for each model
from sklearn.metrics import confusion_matrix
for model_name, model in models.items():
    model.fit(train_X,train_y)
    y_pred = model.predict(test_X)
    cm = confusion_matrix(test_y, y_pred)

    # Create a heatmap for the confusion matrix
    plt.figure(figsize=(6, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix for {model_name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

import time

# Initialize models
models = {
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=random_state),
    'XGBoost': XGBClassifier(random_state=random_state),
    'Gradient Boosting': GradientBoostingClassifier(),
    'SVM': SVC(),
    'Logistic Regression': LogisticRegression(),
    'KNN Classifier': KNeighborsClassifier(),
    'Decision Tree': DecisionTreeClassifier()
}

# Train and evaluate models
for name, model in models.items():
    start_time = time.time()  # Record start time
    model.fit(train_X, train_y)
    y_pred = model.predict(test_X)
    end_time = time.time()  # Record end time

    accuracy = accuracy_score(test_y, y_pred)
    report = classification_report(test_y, y_pred, output_dict=True)

    elapsed_time = end_time - start_time  # Calculate elapsed time

    print(f'{name}:\nAccuracy: {accuracy*100:.3f}%\nElapsed Time: {elapsed_time:.5f} seconds')
    print(f'\n{"="*40}\n')

from sklearn.model_selection import cross_val_score

# Define your machine learning models as before
models = {
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=random_state),
    'XGBoost': XGBClassifier(random_state=random_state),
    'Gradient Boosting': GradientBoostingClassifier(),
    'SVM': SVC(),
    'Logistic Regression': LogisticRegression(),
    'KNN Classifier': KNeighborsClassifier(),
    'Decision Tree': DecisionTreeClassifier()
}

# Perform cross-validation for each model
for name, model in models.items():
    scores = cross_val_score(model, train_X, train_y, cv=5)  # 5-fold cross-validation (adjust as needed)
    mean_accuracy = scores.mean()
    std_accuracy = scores.std()

    print(f'{name} Cross-Validation Results:')
    print(f'Mean Accuracy: {mean_accuracy*100:.3f}%')
    print(f'Standard Deviation of Accuracy: {std_accuracy*100:.3f}%')
    print("=" * 40)

from sklearn.model_selection import cross_val_score, StratifiedKFold

# List of all models including the newly added ones (including Gradient Boosting and SVM)
models = {
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=random_state),
    'XGBoost': XGBClassifier(random_state=random_state),
    'Gradient Boosting': GradientBoostingClassifier(),
    'SVM': SVC(),
    'Logistic Regression': LogisticRegression(),
    'KNN Classifier': KNeighborsClassifier(),
    'Decision Tree': DecisionTreeClassifier()
}

# Perform cross-validation and assess model performance
results = []

for name, model in models.items():
    # Use stratified k-fold cross-validation
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)

    # Perform cross-validation and get accuracy scores
    scores = cross_val_score(model, train_X, train_y, cv=cv, scoring='accuracy')

    # Compute mean and standard deviation of accuracy scores
    mean_accuracy = np.mean(scores)
    std_accuracy = np.std(scores)

    results.append({
        'Model': name,
        'Mean Accuracy': mean_accuracy,
        'Standard Deviation': std_accuracy
    })

# Create a DataFrame for cross-validation results
cv_results_df = pd.DataFrame(results)

# Display cross-validation results
print(cv_results_df)